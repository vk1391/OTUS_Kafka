- name: disable selinux
  command: setenforce 0
- name: create java directory
  command: mkdir /usr/local/jdk
  ignore_errors: true
- name: install some packets
  unarchive:
    src: https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.9%2B9/OpenJDK17U-jdk_x64_linux_hotspot_17.0.9_9.tar.gz
    dest: /usr/local/jdk
    remote_src: yes
- name: create PATH
  copy:
    content: |
      JAVA_HOME=/usr/local/jdk/jdk-17.0.9+9
      PATH=$JAVA_HOME/bin:$PATH
    dest: /etc/profile.d/java.sh 
- name: add firewall kafka rules
  copy:
    content: |
      <?xml version="1.0" encoding="utf-8"?>
      <service>      
      <short>kafka</short>
      <description>Firewall rule for kafka port</description>
      <port protocol="tcp" port="9092"/>
      <port protocol="tcp" port="9093"/>
      </service>
    dest: /etc/firewalld/services/kafka.xml
- name: start firewalld
  systemd:
    name: firewalld
    state: started
    enabled: true
- name: Configuring firewall services
  firewalld:
    service: kafka
    permanent: yes
    state: enabled
    immediate: yes
- name: start firewalld
  systemd:
    name: firewalld
    state: restarted
- name: create user kafka
  command: useradd -r -c 'Kafka broker user service' kafka
  ignore_errors: true
- name: create kafka1 dir
  file:
    path: /opt/kafka
    state: directory
    owner: kafka
    group: kafka
- name: download & put kafka 1
  unarchive:
    src: https://downloads.apache.org/kafka/3.6.1/{{ kafka_vers }}.tgz
    dest: /opt/kafka
    remote_src: yes
    owner: kafka
    group: kafka
- name: add host to etc/hosts
  copy:
    content: |
      127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
      ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
      127.0.1.1 {{ kafka_name }} {{ kafka_name }}
      192.168.56.211 kafka1 kafka1
      192.168.56.212 kafka2 kafka2
      192.168.56.213 kafka3 kafka3
    dest: /etc/hosts
- name: config Kraft server
  copy: 
    content: |
      # Licensed to the Apache Software Foundation (ASF) under one or more
      # contributor license agreements.  See the NOTICE file distributed with
      # this work for additional information regarding copyright ownership.
      # The ASF licenses this file to You under the Apache License, Version 2.0
      # (the "License"); you may not use this file except in compliance with
      # the License.  You may obtain a copy of the License at
      #
      #    http://www.apache.org/licenses/LICENSE-2.0
      #
      # Unless required by applicable law or agreed to in writing, software
      # distributed under the License is distributed on an "AS IS" BASIS,
      # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      # See the License for the specific language governing permissions and
      # limitations under the License.

      #
      # This configuration file is intended for use in KRaft mode, where
      # Apache ZooKeeper is not present.
      #

      ############################# Server Basics #############################

      # The role of this server. Setting this puts us in KRaft mode
      process.roles=broker,controller

      # The node id associated with this instance's roles
      node.id={{ kafka_id }}

      # The connect string for the controller quorum
      controller.quorum.voters=1@kafka1:9093,2@kafka2:9093,3@kafka3:9093

      ############################# Socket Server Settings #############################

      # The address the socket server listens on.
      # Combined nodes (i.e. those with `process.roles=broker,controller`) must list the controller listener here at a minimum.
      # If the broker listener is not defined, the default listener will use a host name that is equal to the value of java.net.InetAddress.getCanonicalHostName(),
      # with PLAINTEXT listener name, and port 9092.
      #   FORMAT:
      #     listeners = listener_name://host_name:port
      #   EXAMPLE:
      #     listeners = PLAINTEXT://your.host.name:9092
      listeners=PLAINTEXT://:9092,CONTROLLER://:9093

      # Name of listener used for communication between brokers.
      inter.broker.listener.name=PLAINTEXT

      # Listener name, hostname and port the broker will advertise to clients.
      # If not set, it uses the value for "listeners".
      advertised.listeners=PLAINTEXT://0.0.0.0:9092

      # A comma-separated list of the names of the listeners used by the controller.
      # If no explicit mapping set in `listener.security.protocol.map`, default will be using PLAINTEXT protocol
      # This is required if running in KRaft mode.
      controller.listener.names=CONTROLLER

      # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
      listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

      # The number of threads that the server uses for receiving requests from the network and sending responses to the network
      num.network.threads=3

      # The number of threads that the server uses for processing requests, which may include disk I/O
      num.io.threads=8

      # The send buffer (SO_SNDBUF) used by the socket server
      socket.send.buffer.bytes=102400

      # The receive buffer (SO_RCVBUF) used by the socket server
      socket.receive.buffer.bytes=102400

      # The maximum size of a request that the socket server will accept (protection against OOM)
      socket.request.max.bytes=104857600


      ############################# Log Basics #############################

      # A comma separated list of directories under which to store log files
      log.dirs=/tmp/kraft-combined-logs

      # The default number of log partitions per topic. More partitions allow greater
      # parallelism for consumption, but this will also result in more files across
      # the brokers.
      num.partitions=3

      # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
      # This value is recommended to be increased for installations with data dirs located in RAID array.
      num.recovery.threads.per.data.dir=1

      ############################# Internal Topic Settings  #############################
      # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
      # For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
      offsets.topic.replication.factor=3
      transaction.state.log.replication.factor=1
      transaction.state.log.min.isr=1

      ############################# Log Flush Policy #############################

      # Messages are immediately written to the filesystem but by default we only fsync() to sync
      # the OS cache lazily. The following configurations control the flush of data to disk.
      # There are a few important trade-offs here:
      #    1. Durability: Unflushed data may be lost if you are not using replication.
      #    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
      #    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
      # The settings below allow one to configure the flush policy to flush data after a period of time or
      # every N messages (or both). This can be done globally and overridden on a per-topic basis.

      # The number of messages to accept before forcing a flush of data to disk
      #log.flush.interval.messages=10000

      # The maximum amount of time a message can sit in a log before we force a flush
      #log.flush.interval.ms=1000

      ############################# Log Retention Policy #############################

      # The following configurations control the disposal of log segments. The policy can
      # be set to delete segments after a period of time, or after a given size has accumulated.
      # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
      # from the end of the log.

      # The minimum age of a log file to be eligible for deletion due to age
      log.retention.hours=168

      # A size-based retention policy for logs. Segments are pruned from the log unless the remaining
      # segments drop below log.retention.bytes. Functions independently of log.retention.hours.
      #log.retention.bytes=1073741824

      # The maximum size of a log segment file. When this size is reached a new log segment will be created.
      log.segment.bytes=1073741824

      # The interval at which log segments are checked to see if they can be deleted according
      # to the retention policies
      log.retention.check.interval.ms=300000
    dest: /opt/kafka/{{ kafka_vers }}/config/kraft/server.properties
- name: generate ids
  shell: /opt/kafka/{{ kafka_vers }}/bin/kafka-storage.sh random-uuid
  register: cluster_id
- name: copy cluster_id in config
  shell: "{{ item }}"
  with_items: 
    - /opt/kafka/{{ kafka_vers }}/bin/kafka-storage.sh format -t {{ hostvars.kafka1.cluster_id.stdout }} -c /opt/kafka/{{ kafka_vers }}/config/kraft/server.properties
  ignore_errors: true
- name: start kafka
  shell: "{{ item }}"
  with_items: 
    - /opt/kafka/{{ kafka_vers }}/bin/kafka-server-start.sh -daemon /opt/kafka/{{ kafka_vers }}/config/kraft/server.properties
- name: create topics
  command: "{{ item }}"
  with_items: 
    - /opt/kafka/kafka_2.13-3.6.1/bin/kafka-topics.sh --create --bootstrap-server kafka1:9092 --replication-factor 2 --partitions 2 --topic nginx
    - /opt/kafka/kafka_2.13-3.6.1/bin/kafka-topics.sh --create --bootstrap-server kafka1:9092 --replication-factor 2 --partitions 2 --topic wordpress